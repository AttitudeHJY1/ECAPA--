{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create loss dict\n",
      "loss dict created\n",
      "Epoch:1,Loss:1.7629191743002997,Accuracy:0.26181818181818184,\n",
      "Epoch:2,Loss:1.705138921737671,Accuracy:0.29818181818181816,\n",
      "Epoch:3,Loss:1.6527327564027574,Accuracy:0.4,\n",
      "Epoch:4,Loss:1.5989306105507746,Accuracy:0.4381818181818182,\n",
      "Epoch:5,Loss:1.5412685208850436,Accuracy:0.4690909090909091,\n",
      "Epoch:6,Loss:1.4965039491653442,Accuracy:0.4709090909090909,\n",
      "Epoch:7,Loss:1.453702449798584,Accuracy:0.48727272727272725,\n",
      "Epoch:8,Loss:1.4006497197681003,Accuracy:0.5272727272727272,\n",
      "Epoch:9,Loss:1.3635508881674872,Accuracy:0.5363636363636364,\n",
      "Epoch:10,Loss:1.312432262632582,Accuracy:0.5309090909090909,\n",
      "Epoch:11,Loss:1.261875483724806,Accuracy:0.5363636363636364,\n",
      "Epoch:12,Loss:1.238268772761027,Accuracy:0.5709090909090909,\n",
      "Epoch:13,Loss:1.185283899307251,Accuracy:0.5781818181818181,\n",
      "Epoch:14,Loss:1.1392408344480727,Accuracy:0.610909090909091,\n",
      "Epoch:15,Loss:1.1061904297934637,Accuracy:0.5945454545454546,\n",
      "Epoch:16,Loss:1.0728545718722873,Accuracy:0.6218181818181818,\n",
      "Epoch:17,Loss:1.0495394600762262,Accuracy:0.6272727272727273,\n",
      "Epoch:18,Loss:1.0152740412288241,Accuracy:0.66,\n",
      "Epoch:19,Loss:0.9782268073823717,Accuracy:0.6454545454545455,\n",
      "Epoch:20,Loss:0.9591078228420682,Accuracy:0.6581818181818182,\n",
      "Epoch:21,Loss:0.9258967902925279,Accuracy:0.6472727272727272,\n",
      "Epoch:22,Loss:0.9278925591044955,Accuracy:0.649090909090909,\n",
      "Epoch:23,Loss:0.8883017235332065,Accuracy:0.6745454545454546,\n",
      "Epoch:24,Loss:0.8712268869082133,Accuracy:0.6709090909090909,\n",
      "Epoch:25,Loss:0.8409971859720018,Accuracy:0.7090909090909091,\n",
      "Epoch:26,Loss:0.7889431648784213,Accuracy:0.7272727272727273,\n",
      "Epoch:27,Loss:0.8080885145399306,Accuracy:0.7163636363636363,\n",
      "Epoch:28,Loss:0.8004012107849121,Accuracy:0.7072727272727273,\n",
      "Epoch:29,Loss:0.8015925619337294,Accuracy:0.7272727272727273,\n",
      "Epoch:30,Loss:0.7554723156823052,Accuracy:0.7236363636363636,\n",
      "Epoch:31,Loss:0.7354930109447904,Accuracy:0.7272727272727273,\n",
      "Epoch:32,Loss:0.7141511042912801,Accuracy:0.7563636363636363,\n",
      "Epoch:33,Loss:0.6654599441422356,Accuracy:0.76,\n",
      "Epoch:34,Loss:0.6814580029911466,Accuracy:0.7563636363636363,\n",
      "Epoch:35,Loss:0.6618883444203271,Accuracy:0.7836363636363637,\n",
      "Epoch:36,Loss:0.6107769376701779,Accuracy:0.7909090909090909,\n",
      "Epoch:37,Loss:0.6264623138639662,Accuracy:0.7727272727272727,\n",
      "Epoch:38,Loss:0.6030005580849118,Accuracy:0.7818181818181819,\n",
      "Epoch:39,Loss:0.554627411895328,Accuracy:0.8163636363636364,\n",
      "Epoch:40,Loss:0.5782668623659346,Accuracy:0.7872727272727272,\n",
      "Epoch:41,Loss:0.5948557290765975,Accuracy:0.7854545454545454,\n",
      "Epoch:42,Loss:0.5504571795463562,Accuracy:0.8109090909090909,\n",
      "Epoch:43,Loss:0.529162691699134,Accuracy:0.7981818181818182,\n",
      "Epoch:44,Loss:0.5181239214208391,Accuracy:0.8127272727272727,\n",
      "Epoch:45,Loss:0.5011898080507914,Accuracy:0.82,\n",
      "Epoch:46,Loss:0.4368147949377696,Accuracy:0.84,\n",
      "Epoch:47,Loss:0.4636828965610928,Accuracy:0.8163636363636364,\n",
      "Epoch:48,Loss:0.43704166014989215,Accuracy:0.8454545454545455,\n",
      "Epoch:49,Loss:0.46713053186734516,Accuracy:0.8327272727272728,\n",
      "Epoch:50,Loss:0.47655348976453143,Accuracy:0.8272727272727273,\n",
      "eval_step--:,Epoch:50,Accuracy:0.8484848484848485,Micro_f1:0.8484848484848486,Total items:66,\n",
      "---------------- model saved ------------------- \n",
      "Epoch:51,Loss:0.3789741098880768,Accuracy:0.8745454545454545,\n",
      "Epoch:52,Loss:0.39805367920133805,Accuracy:0.8745454545454545,\n",
      "Epoch:53,Loss:0.35664041837056476,Accuracy:0.8727272727272727,\n",
      "Epoch:54,Loss:0.38514912459585404,Accuracy:0.8745454545454545,\n",
      "Epoch:55,Loss:0.3437532451417711,Accuracy:0.88,\n",
      "Epoch:56,Loss:0.41686081555154586,Accuracy:0.86,\n",
      "Epoch:57,Loss:0.40876879956987167,Accuracy:0.8381818181818181,\n",
      "Epoch:58,Loss:0.3316280510690477,Accuracy:0.8854545454545455,\n",
      "Epoch:59,Loss:0.32057804531521267,Accuracy:0.889090909090909,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "import math, random \n",
    "import torch \n",
    "import torchaudio \n",
    "from torchaudio import transforms \n",
    "from IPython.display import Audio \n",
    "from torch.utils.data import DataLoader, Dataset, random_split \n",
    "import torch.nn.functional as F \n",
    "from torch.nn import init \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import glob, os\n",
    "from scipy.io import wavfile\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.chdir(\"D:\\Pytorch_train\");\n",
    "#读取csv文件 \n",
    "download_path = Path.cwd()/'hj_data/train' \n",
    "noise_dir = Path.cwd()/'free-sound'\n",
    "# Read metadata file \n",
    "metadata_file = download_path/'data.csv' \n",
    "traindata_file = download_path/'train.csv'\n",
    "evaldata_file = download_path/'eval.csv'\n",
    "df = pd.read_csv(metadata_file) \n",
    "df.head() \n",
    " \n",
    "# Construct file path by concatenating fold and file name \n",
    "df['relative_path'] = '/' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str) \n",
    "# train_df['relative_path'] = '/' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str) \n",
    "# eval_df['relative_path'] = '/' + df['fold'].astype(str) + '/' + df['slice_file_name'].astype(str) \n",
    " \n",
    "# Take relevant columns \n",
    "df = df[['relative_path', 'classID']] \n",
    "# train_df = train_df[['relative_path', 'classID']] \n",
    "# eval_df = eval_df[['relative_path', 'classID']] \n",
    "df.head()\n",
    "\n",
    "#读取文件中的音频\n",
    "# import math, random \n",
    "# import torch \n",
    "# import torchaudio \n",
    "# from torchaudio import transforms \n",
    "# from IPython.display import Audio \n",
    " \n",
    "class AudioUtil(): \n",
    "  # ---------------------------- \n",
    "  # Load an audio file. Return the signal as a tensor and the sample rate \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  #audio_file = download_path/'fold1'/'101415-3-0-2.wav'\n",
    "  def open(audio_file): \n",
    "    sig, sr = torchaudio.load(audio_file) \n",
    "    return (sig, sr)\n",
    "\n",
    "#转换为立体声\n",
    "# ---------------------------- 未执行\n",
    "  # Convert the given audio to the desired number of channels \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  def rechannel(aud, new_channel): \n",
    "    sig, sr = aud \n",
    " \n",
    "    if (sig.shape[0] == new_channel): \n",
    "      # Nothing to do \n",
    "      return aud \n",
    " \n",
    "    if (new_channel == 1): \n",
    "      # Convert from stereo to mono by selecting only the first channel \n",
    "      resig = sig[:1, :] \n",
    "    else: \n",
    "      # Convert from mono to stereo by duplicating the first channel \n",
    "      resig = torch.cat([sig, sig]) \n",
    " \n",
    "    return ((resig, sr))\n",
    "\n",
    "#标准化采样率\n",
    "# ---------------------------- \n",
    "  # Since Resample applies to a single channel, we resample one channel at a time \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  def resample(aud, newsr): \n",
    "    sig, sr = aud \n",
    " \n",
    "    if (sr == newsr): \n",
    "      # Nothing to do \n",
    "      return aud \n",
    " \n",
    "    num_channels = sig.shape[0] \n",
    "    # Resample first channel \n",
    "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:]) \n",
    "    if (num_channels > 1): \n",
    "      # Resample the second channel and merge both channels \n",
    "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:]) \n",
    "      resig = torch.cat([resig, retwo]) \n",
    " \n",
    "    return ((resig, newsr))\n",
    "\n",
    "#调整为相同长度\n",
    "# ---------------------------- \n",
    "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds \n",
    "  # 使用ms为单位进行固定长度，而不是帧长\n",
    "  @staticmethod \n",
    "  def pad_trunc(aud, max_ms): \n",
    "    sig, sr = aud \n",
    "#     print(\"sig_shape:\".format(sig.shape))\n",
    "    num_rows, sig_len = sig.shape \n",
    "#     print(f\"sig_len:{sig_len}\")\n",
    "    max_len = sr//1000 * max_ms \n",
    " \n",
    "    if (sig_len > max_len): \n",
    "      # Truncate the signal to the given length \n",
    "      sig = sig[:,:max_len] \n",
    " \n",
    "    elif (sig_len < max_len): \n",
    "      # Length of padding to add at the beginning and end of the signal \n",
    "      pad_begin_len = random.randint(0, max_len - sig_len) \n",
    "      pad_end_len = max_len - sig_len - pad_begin_len \n",
    " \n",
    "      # Pad with 0s \n",
    "      pad_begin = torch.zeros((num_rows, pad_begin_len)) \n",
    "      pad_end = torch.zeros((num_rows, pad_end_len)) \n",
    " \n",
    "      sig = torch.cat((pad_begin, sig, pad_end), 1) \n",
    "#     print(f\"sig_shape_after:{sig.shape}\")\n",
    "    return (sig, sr)\n",
    "\n",
    "#数据扩充增广（时移）\n",
    "# ---------------------------- \n",
    "  # Shifts the signal to the left or right by some percent. Values at the end \n",
    "  # are 'wrapped around' to the start of the transformed signal. \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  def time_shift(aud, shift_limit): \n",
    "    sig,sr = aud \n",
    "#     print(f\"afterpad_sig_shape:{sig.shape}\")#[2, 220000]\n",
    "    _, sig_len = sig.shape \n",
    "    shift_amt = int(random.random() * shift_limit * sig_len) \n",
    "    return (sig.roll(shift_amt), sr)\n",
    "\n",
    "#梅尔谱图\n",
    "# ---------------------------- \n",
    "  # Generate a Spectrogram \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None): \n",
    "    sig,sr = aud \n",
    "#     print(f\"after_timeshift_sig_shape:{sig.shape}\")#[2, 220000]维度不变\n",
    "    top_db = 80 \n",
    " \n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc \n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig) \n",
    "#     print(f\"spec_shape:{spec.shape}\")#[2, 64, 430] n_mels=64, n_fft=1024, hop_len=None\n",
    "    # Convert to decibels \n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec) \n",
    "    return (spec)\n",
    "  \n",
    "#   @staticmethod \n",
    "#   def GaussNoise(spectrogram):\n",
    "#       if np.random.random() > self.add_noise_prob:\n",
    "#           return spectrogram\n",
    "\n",
    "#       # set the std value \n",
    "#       min_pixel_value = np.min(spectrogram)\n",
    "#       if self.std is None:\n",
    "#         std_factor = 0.03     # factor number \n",
    "#         std = np.abs(min_pixel_value*std_factor)\n",
    "\n",
    "#       # generate a white noise spectrogram\n",
    "#       gauss_mask = np.random.normal(self.mean, \n",
    "#                                     std, \n",
    "#                                     size=self.input_size).astype('float32')\n",
    "      \n",
    "#       # add white noise to the sound spectrogram\n",
    "#       noisy_spectrogram = spectrogram + gauss_mask\n",
    "\n",
    "#       return noisy_spectrogram\n",
    "\n",
    "#数据扩充：时间和频率屏蔽\n",
    "# ---------------------------- \n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency \n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent \n",
    "  # overfitting and to help the model generalise better. The masked sections are \n",
    "  # replaced with the mean value. \n",
    "  # ---------------------------- \n",
    "  @staticmethod \n",
    "  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1): \n",
    "    _, n_mels, n_steps = spec.shape \n",
    "    mask_value = spec.mean() \n",
    "    aug_spec = spec \n",
    " \n",
    "    freq_mask_param = max_mask_pct * n_mels \n",
    "    for _ in range(n_freq_masks): \n",
    "      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value) \n",
    " \n",
    "    time_mask_param = max_mask_pct * n_steps \n",
    "    for _ in range(n_time_masks): \n",
    "      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value) \n",
    " \n",
    "    return aug_spec\n",
    "\n",
    "  #加噪（musan）\n",
    "  @staticmethod \n",
    "  def noise_augmentation(wave, noise_dir):\n",
    "    \"\"\" Perform noise augmentation of the wave by loading three noise segments\n",
    "    from the noise_dir and add these on top of the wave with a dampening factor\n",
    "    of 0.4\n",
    "    \"\"\"\n",
    "    noise_paths = glob.glob(os.path.join(noise_dir, \"*.wav\"))\n",
    "    aug_noise_paths = np.random.choice(noise_paths, 1, replace=False)\n",
    "    print(aug_noise_paths)\n",
    "    aug_noise_paths = str(aug_noise_paths[0])\n",
    "    print(aug_noise_paths)\n",
    "    #(fs, aug_noise) = wavfile.read(aug_noise_path)\n",
    "    aug_noise = AudioUtil.open(aug_noise_paths)\n",
    "    aug_noise = AudioUtil.resample(aug_noise, 22050) #标准化采样率\n",
    "    aug_noise = AudioUtil.rechannel(aug_noise, 2) #转换为立体声，统一为两个声道\n",
    "    aug_noise = AudioUtil.pad_trunc(aug_noise, 10000) #调整为相同长度\n",
    "    #print(f'wave:{wave}, aug_noise:{aug_noise}')\n",
    "    dampening_factor = 0.4\n",
    "    for aug_noise_path in aug_noise_paths:\n",
    "        aug_noise = list(aug_noise)\n",
    "        wave = list(wave)\n",
    "#         initial_wave = np.concatenate(wave)\n",
    "        \n",
    "#         print(f'wave1:{wave}, aug_noise1:{aug_noise}')\n",
    "#         aug_noise = torch.tensor([item.detach().numpy() for item in aug_noise])\n",
    "        wave[0] = wave[0] + aug_noise[0]*dampening_factor\n",
    "#         print(f'wave2:{wave}, aug_noise2:{aug_noise}')\n",
    "#         wave = torch.from_numpy(wave)\n",
    "#         print(f'wave:{wave}')\n",
    "    return wave\n",
    "\n",
    "#自定义数据加载器\n",
    "# from torch.utils.data import DataLoader, Dataset, random_split \n",
    "# import torchaudio \n",
    " \n",
    "# ---------------------------- \n",
    "# Sound Dataset \n",
    "# ---------------------------- \n",
    "# SoundDS：MyDataset类\n",
    "\n",
    "\n",
    "class SoundDS(Dataset): \n",
    "  def __init__(self, df, data_path): \n",
    "    self.df = df #df为csv文件\n",
    "    self.data_path = str(data_path) \n",
    "    self.duration = 10000 #固定长度，单位为ms\n",
    "    self.sr = 22050 #采样率\n",
    "    self.channel = 2 #通道数\n",
    "    self.shift_pct = 0.4        \n",
    " \n",
    "  # ---------------------------- \n",
    "  # Number of items in dataset \n",
    "  # ---------------------------- \n",
    "  def __len__(self): \n",
    "    return len(self.df)     \n",
    " \n",
    "  # ---------------------------- \n",
    "  # Get i'th item in dataset \n",
    "  # ---------------------------- \n",
    "  def __getitem__(self, idx): \n",
    "    # Absolute file path of the audio file - concatenate the audio directory with \n",
    "    # the relative path \n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']  #.loc:取idx对应行的所有数据，.loc[idx, 'relative_path']:取idx对应行的relative_path\n",
    "#     print(audio_file)\n",
    "    # Get the Class ID \n",
    "    class_id = self.df.loc[idx, 'classID'] #取idx对应行的classID\n",
    " \n",
    "    aud = AudioUtil.open(audio_file) \n",
    "#     print(f'aud:{aud}')\n",
    "#     AudioUtil.show_wave(aud)\n",
    "    # 有些声音有更高的采样率，或者比大多数声音更少的通道。所以让所有声音都有相同数量的通道和相同的采样率。除非采样速率相同，否则pad_trunc仍然会产生不同长度的数组，即使声音持续时间相同。\n",
    "    reaud = AudioUtil.resample(aud, self.sr) #标准化采样率\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel) #转换为立体声，统一为两个声道\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration) #调整为相同长度\n",
    "#     print(dur_aud)\n",
    "#     print(f'dur_aud:{dur_aud}')\n",
    "#     noise_aud = AudioUtil.noise_augmentation(dur_aud, noise_dir) #加噪\n",
    "#     print(f'noist_aud:{noise_aud}')\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) #时移\n",
    "#     print(f'shift_aud:{shift_aud}')\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None) #转换为mel谱图\n",
    "#     print(f'sgram:{sgram}')\n",
    "    \n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) #时间和频率屏蔽\n",
    "#     print(f'aug_agram:{aug_sgram}')\n",
    "#     print(\"mel-length:{}\".format(aug_sgram.shape))\n",
    "#     print(aug_sgram)\n",
    "#     #画mel谱图\n",
    "#     librosa.display.specshow(mel_spect, sr=fs, x_axis='time', y_axis='mel')\n",
    "#     plt.ylabel('Mel Frequency')\n",
    "#     plt.xlabel('Time(s)')\n",
    "#     plt.title('Mel Spectrogram')\n",
    "#     plt.show()\n",
    "    \n",
    "    return aug_sgram, class_id\n",
    "\n",
    "class eval_SoundDS(Dataset): \n",
    "  def __init__(self, df, data_path): \n",
    "    self.df = df #df为csv文件\n",
    "    self.data_path = str(data_path) \n",
    "    self.duration = 10000 #固定长度，单位为ms\n",
    "    self.sr = 22050 #采样率\n",
    "    self.channel = 2 #通道数\n",
    "    self.shift_pct = 0.4        \n",
    " \n",
    "  # ---------------------------- \n",
    "  # Number of items in dataset \n",
    "  # ---------------------------- \n",
    "  def __len__(self): \n",
    "    return len(self.df)     \n",
    " \n",
    "  # ---------------------------- \n",
    "  # Get i'th item in dataset \n",
    "  # ---------------------------- \n",
    "  def __getitem__(self, idx): \n",
    "    # Absolute file path of the audio file - concatenate the audio directory with \n",
    "    # the relative path \n",
    "    audio_file = self.data_path + self.df.loc[idx, 'relative_path']  #.loc:取idx对应行的所有数据，.loc[idx, 'relative_path']:取idx对应行的relative_path\n",
    "    # Get the Class ID \n",
    "    class_id = self.df.loc[idx, 'classID'] #取idx对应行的classID\n",
    " \n",
    "    aud = AudioUtil.open(audio_file) \n",
    "    # 有些声音有更高的采样率，或者比大多数声音更少的通道。所以让所有声音都有相同数量的通道和相同的采样率。除非采样速率相同，否则pad_trunc仍然会产生不同长度的数组，即使声音持续时间相同。\n",
    "    reaud = AudioUtil.resample(aud, self.sr) #标准化采样率\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel) #转换为立体声，统一为两个声道\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration) #调整为相同长度\n",
    "    sgram = AudioUtil.spectro_gram(dur_aud, n_mels=64, n_fft=1024, hop_len=None) #转换为mel谱图\n",
    "   \n",
    "    return sgram, class_id\n",
    "\n",
    "#使用数据加载器准备一批数据\n",
    "from torch.utils.data import random_split \n",
    "\n",
    "from pathlib import Path \n",
    "download_path = Path.cwd()/'hj_data/train' \n",
    "\n",
    "metadata_file = download_path/'data.csv' \n",
    "traindata_file = download_path/'data.csv'\n",
    "evaldata_file = download_path/'val.csv'\n",
    "# def read_data(test_data=download_path/'data.csv', n=0, label=1):\n",
    "#     '''\n",
    "#     加载数据的功能\n",
    "#     n:特征数据起始位\n",
    "#     label：是否是监督样本数据\n",
    "#     '''\n",
    "#     csv.field_size_limit(500 * 1024 * 1024) #一定要加上这一句\n",
    "#     csv_reader = csv.reader(open(test_data, encoding=\"utf8\", errors=\"ignore\"))\n",
    "#     data_list = []\n",
    "#     for one_line in csv_reader:\n",
    "#         data_list.append(one_line)\n",
    "#     x_list = []\n",
    "#     y_list = []\n",
    "#     for one_line in data_list[1:]:\n",
    "#         if label == 1:#如果是监督样本数据\n",
    "#             y_list.append(int(one_line[-1]))  # 标志位(最后一位都是标签位)\n",
    "#             one_list = [o for o in one_line[n:-1]]\n",
    "#             x_list.append(one_list)\n",
    "#         else:\n",
    "#             one_list = [o for o in one_line[n:]]\n",
    "#             x_list.append(one_list)\n",
    "#     return x_list, y_list\n",
    " \n",
    "# def split_data(data_list, y_list, ratio=0.10):#70%训练集，30%测试集: 914285,391837\n",
    "#     '''\n",
    "#     按照指定的比例，划分样本数据集\n",
    "#     ratio: 测试数据的比率\n",
    "#     '''\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data_list, y_list, test_size=ratio, random_state=50)\n",
    " \n",
    "#     \"\"\"训练集\"\"\"\n",
    "#     if traindata_file == True:\n",
    "#             traindata_file.unlink()  ## 删除已经存在的train.csv文件 (方便下一个epoch重新划分数据集)\n",
    "#     with open(traindata_file, 'w', encoding=\"utf8\",newline=\"\", errors=\"ignore\") as csvfile:#不加newline=\"\"的话会空一行出来\n",
    "#         fieldnames = ['slice_file_name', 'fold','class','classID']\n",
    "#         write = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "#         write.writeheader()#写表头\n",
    "#         for i in range(len(X_train)):\n",
    "#            write.writerow({'slice_file_name':X_train[i][0],'fold':X_train[i][1],'class':X_train[i][2],'classID':y_train[i]})\n",
    " \n",
    "#     \"\"\"测试集\"\"\"\n",
    "#     #标签文件\n",
    "# #     with open('input/sub_test_y', 'w') as fp:\n",
    "# #         json.dump(y_test, fp)\n",
    "#     #测试csv\n",
    "#     if evaldata_file == True:\n",
    "#             evaldata_file.unlink()  ## 删除已经存在的train.csv文件 (方便下一个epoch重新划分数据集)\n",
    "#     with open(evaldata_file, 'w', encoding=\"utf8\",newline=\"\", errors=\"ignore\") as csvfile:#不加newline=\"\"的话会空一行出来\n",
    "#         fieldnames = ['slice_file_name', 'fold','classID','class']\n",
    "#         write = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "#         write.writeheader()#写表头\n",
    "#         for i in range(len(X_test)):\n",
    "#            write.writerow({'slice_file_name':X_train[i][0],'fold':X_train[i][1],'classID':X_train[i][2],'classID':y_train[i]})\n",
    "#     return X_train, X_test, y_train, y_test\n",
    " \n",
    "\n",
    "# \"\"\"获取大文件的数据\"\"\"\n",
    "# x_list, y_list=read_data()\n",
    "# \"\"\"划分为训练集和测试集及label文件\"\"\"\n",
    "# split_data(x_list,y_list)\n",
    "\n",
    " \n",
    "train_df = pd.read_csv(traindata_file)\n",
    "# if traindata_file==NULL or True:\n",
    "#     \"\"\"获取大文件的数据\"\"\"\n",
    "#     x_list, y_list=read_data()\n",
    "#     \"\"\"划分为训练集和测试集及label文件\"\"\"\n",
    "#     split_data(x_list,y_list)\n",
    "    \n",
    "train_df.head() \n",
    "\n",
    "train_df['relative_path'] = '/' + train_df['fold'].astype(str) + '/' + train_df['slice_file_name'].astype(str) \n",
    "train_df = train_df[['relative_path', 'classID']] \n",
    "\n",
    "eval_df = pd.read_csv(evaldata_file) \n",
    "eval_df.head()\n",
    "\n",
    "eval_df['relative_path'] = '/' + eval_df['fold'].astype(str) + '/' + eval_df['slice_file_name'].astype(str) \n",
    "\n",
    "eval_df = eval_df[['relative_path', 'classID']] \n",
    "\n",
    "train_ds = SoundDS(train_df,download_path)\n",
    "# train_ds2 = eval_SoundDS(train_df,download_path) \n",
    "# train_ds.Merge(train_ds2,true,MissingSchemaAction.AddWithKey);\n",
    "\n",
    "eval_ds = eval_SoundDS(eval_df,download_path) \n",
    " \n",
    " \n",
    "# Create training and validation data loaders \n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True) \n",
    "val_dl = torch.utils.data.DataLoader(eval_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "#建立模型\n",
    "# import torch.nn.functional as F \n",
    "# from torch.nn import init \n",
    " \n",
    "# ---------------------------- \n",
    "# Audio Classification Model \n",
    "# ---------------------------- \n",
    "class AudioClassifier (nn.Module): \n",
    "    # ---------------------------- \n",
    "    # Build the model architecture \n",
    "    # ---------------------------- \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "        conv_layers = [] \n",
    " \n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization \n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) #in_channels=2,out_channels=8\n",
    "        self.relu1 = nn.PReLU() \n",
    "        self.bn1 = nn.BatchNorm2d(8) \n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1) #a-此层之后使用的整流器的负斜率(仅与 'leaky_relu' 一起使用)\n",
    "        self.conv1.bias.data.zero_() \n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1] \n",
    " \n",
    "        # Second Convolution Block \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) \n",
    "        self.relu2 = nn.PReLU() \n",
    "        self.bn2 = nn.BatchNorm2d(16) \n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1) \n",
    "        self.conv2.bias.data.zero_() \n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2] \n",
    " \n",
    "        # third Convolution Block \n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) \n",
    "        self.relu3 = nn.PReLU() \n",
    "        self.bn3 = nn.BatchNorm2d(32) \n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1) \n",
    "        self.conv3.bias.data.zero_() \n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3] \n",
    " \n",
    "        # fourth Convolution Block \n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) \n",
    "        self.relu4 = nn.PReLU() \n",
    "        self.bn4 = nn.BatchNorm2d(64) \n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1) \n",
    "        self.conv4.bias.data.zero_() \n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4] \n",
    "         \n",
    "        # fifth Convolution Block \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) \n",
    "        self.relu5 = nn.PReLU() \n",
    "        self.bn5 = nn.BatchNorm2d(128) \n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1) \n",
    "        self.conv5.bias.data.zero_() \n",
    "        conv_layers += [self.conv5, self.relu5, self.bn5] \n",
    "        \n",
    "        # sixth Convolution Block \n",
    "        self.conv6 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) \n",
    "        self.relu6 = nn.PReLU() \n",
    "        self.bn6 = nn.BatchNorm2d(256) \n",
    "        init.kaiming_normal_(self.conv6.weight, a=0.1) \n",
    "        self.conv6.bias.data.zero_() \n",
    "        conv_layers += [self.conv6, self.relu6, self.bn6] \n",
    "        \n",
    "        # Linear Classifier \n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1) \n",
    "        self.lin = nn.Linear(in_features=256, out_features=6) \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Conv2d(256,256//16,kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256//16,256,kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    " \n",
    "        # Wrap the Convolutional Blocks 打包卷积块\n",
    "        self.conv = nn.Sequential(*conv_layers) \n",
    " \n",
    "    # ---------------------------- \n",
    "    # Forward pass computations \n",
    "    # ---------------------------- \n",
    "    def forward(self, x): \n",
    "        # Run the convolutional blocks \n",
    "        x = self.conv(x) \n",
    "        \n",
    "        x_se = self.se(x)\n",
    "        x = x * x_se\n",
    " \n",
    "        # Adaptive pool and flatten for input to linear layer \n",
    "        x = self.ap(x) \n",
    "        x = x.view(x.shape[0], -1) #=reshape 重新定义矩阵形状，-1代表动态调整这个维度上的元素个数，以保证元素的总数不变\n",
    " \n",
    "        # Linear layer \n",
    "        logits1 = self.lin(self.dropout1(x))\n",
    "        logits2 = self.lin(self.dropout2(x))\n",
    "        logits3 = self.lin(self.dropout3(x))\n",
    "        logits4 = self.lin(self.dropout4(x))\n",
    "        logits5 = self.lin(self.dropout5(x))\n",
    "\n",
    "        x = (logits1 + logits2 + logits3 + logits4 + logits5) / 5\n",
    "#         x = self.lin(x) \n",
    " \n",
    "        # Final output \n",
    "        return x \n",
    " \n",
    "# Create the model and put it on the GPU if available \n",
    "myModel = AudioClassifier() \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device) \n",
    "# Check that it is on Cuda \n",
    "next(myModel.parameters()).device\n",
    "\n",
    "#训练\n",
    "# ---------------------------- \n",
    "# Training Loop \n",
    "# ---------------------------- \n",
    "def training(model, train_dl, num_epochs): \n",
    "  # Loss Function, Optimizer and Scheduler \n",
    "  \n",
    "  def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    # device = torch.device('cpu')\n",
    "    # # preds = preds.to(device)\n",
    "    # # truths = preds.to(device)\n",
    "    # preds.cpu()\n",
    "    # truths.cpu()\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds,average='micro')\n",
    "\n",
    "#   def logf1_loss(truths,preds):\n",
    "#         f1_score = micro_f1(preds, truths)\n",
    "#         loss = -(np.log(f1_score))\n",
    "#         loss = torch.tensor([loss], requires_grad=True)\n",
    "#         return loss\n",
    "\n",
    "\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss() \n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001) \n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, \n",
    "                                                steps_per_epoch=int(len(train_dl)), \n",
    "                                                epochs=num_epochs, \n",
    "                                                anneal_strategy='linear') #退火策略，'cos'和'liner'分别表示余弦退火和线性退火\n",
    " \n",
    "  expdir = Path.cwd()/'hj_data/experiments'\n",
    "  version = 'v11'\n",
    "  logfilepath = expdir / version / 'logs_{}.txt'.format(version)\n",
    "  logfilepath_eval = expdir / version / 'logs_eval_{}.txt'.format(version)\n",
    "  model_savedir = expdir / version \n",
    "  def init_logger(self):\n",
    "        self.logFileName = str(self.logfilepath)\n",
    "        if self.logfilepath.exists == True:\n",
    "            self.logfilepath.exists.unlink()  ## 删除已经存在的log文件 (方便重启实验)\n",
    "        if self.logfilepath_eval.exists == True:\n",
    "            self.logfilepath_eval.exists.unlink()  ## 删除已经存在的log文件 (方便重启实验)\n",
    "        self.logFileName_eval = str(self.logfilepath_eval)\n",
    "\n",
    "        ## 存储training log文件\n",
    "        with open(self.logFileName, 'a', encoding='utf-8') as wf:\n",
    "            # #将参数类中的所有参数写入log\n",
    "            # for k, v in self.hp.__dict__.items():\n",
    "            #     wf.write(\"{} : {}\\n\".format(k, v))\n",
    "            # wf.write('-' * 50 + \"Experiment & Hparams Created\" + \"-\" * 50 + \"\\n\")\n",
    "            wf.write(\"*\" * 100 + \"\\n\")\n",
    "            wf.close()\n",
    "\n",
    "    ## 该方法将一个字典  ，按kv的顺序，写入一行到 log.txt\n",
    "  def write_line2log(log_dict: dict, filedir, isprint: True):\n",
    "      strp = ''\n",
    "      with open(filedir, 'a', encoding='utf-8') as f:\n",
    "          for key, value in log_dict.items():\n",
    "              witem = '{}'.format(key) + ':{},'.format(value)\n",
    "              strp += witem\n",
    "          f.write(strp)\n",
    "          #f.write('当前进程的内存使用：%.4f GB' % (psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024 / 1024))\n",
    "          f.write('\\n')\n",
    "      if isprint:\n",
    "          print(strp)\n",
    "      pass\n",
    "\n",
    "  def print_network(model, name):\n",
    "      \"\"\"Print out the network information.\"\"\"\n",
    "      num_params = 0\n",
    "      for p in model.parameters():\n",
    "          num_params += p.numel()\n",
    "      print(\"Model {},the number of parameters: {}\".format(name, num_params))\n",
    "\n",
    "  #保存模型\n",
    "  def save_model(i):\n",
    "      pdict = {\"model\":model.state_dict(),\n",
    "                }\n",
    "      path = model_savedir / \"{:04}.pth\".format(i)\n",
    "      torch.save(pdict, str(path))\n",
    "      print(\"---------------- model saved ------------------- \")\n",
    "  \n",
    "\n",
    "\n",
    "  #测试\n",
    "  def test_acc (model, val_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    truths = []\n",
    "    preds = []\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "      for data in val_dl:\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "#         print(f'labes: {labels},preds: {prediction}')\n",
    "        # state = torch.device('cpu')\n",
    "        # labels = labels.to(device)\n",
    "        # prediction = prediction.to(device)\n",
    "        labels = labels.cpu()\n",
    "        prediction = prediction.cpu()\n",
    "#         print(f'labes: {labels},preds: {prediction}')\n",
    "        truths.append(labels)\n",
    "        preds.append(prediction)\n",
    "\n",
    "    \n",
    "    acc = correct_prediction/total_prediction\n",
    "    f1 = micro_f1(truths,preds)\n",
    "    losse_curves  = {\"eval_step--\":\"\",\n",
    "                    \"Epoch\":epoch,\n",
    "                    \"Accuracy\":acc,\n",
    "                    \"Micro_f1\":f1,\n",
    "                    \"Total items\":total_prediction}\n",
    "    write_line2log(losse_curves, logfilepath_eval, isprint=True)\n",
    "\n",
    "\n",
    "  # Repeat for each epoch \n",
    "  for epoch in range(1,num_epochs): \n",
    "    running_loss = 0.0 \n",
    "    correct_prediction = 0 \n",
    "    total_prediction = 0 \n",
    " \n",
    "    # Repeat for each batch in the training set \n",
    "    for i, data in enumerate(train_dl):\n",
    "#         data[0] = list(data[0])\n",
    "#         data[0][1] = data[0][1].item()\n",
    "#         data[0][0] = data[0][0].numpy().tolist()\n",
    "#         data[0][0][0] = list(data[0][0][0])\n",
    "#         print(data[0])\n",
    "#         if i%2 == 0:\n",
    "#             data[0] = AudioUtil.noise_augmentation(data[0], noise_dir) #加噪\n",
    "#             data[0] = AudioUtil.time_shift(data[0], 0.4) #时移\n",
    "#             #     print(f'shift_aud:{shift_aud}')\n",
    "#             data[0] = AudioUtil.spectro_gram(data[0], n_mels=64, n_fft=1024, hop_len=None) #转换为mel谱图\n",
    "#             #     print(f'sgram:{sgram}')\n",
    "#             data[0] = AudioUtil.spectro_augment(data[0], max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) #时间和频率屏蔽\n",
    "#         else:\n",
    "#             data[0] = AudioUtil.spectro_gram(data[0], n_mels=64, n_fft=1024, hop_len=None) #转换为mel谱图\n",
    "        # Get the input features and target labels, and put them on the GPU \n",
    "        inputs, labels = data[0].to(device), data[1].to(device) \n",
    " \n",
    "        # Normalize the inputs \n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std() #标准差\n",
    "        inputs = (inputs - inputs_m) / inputs_s \n",
    " \n",
    "        # Zero the parameter gradients \n",
    "        optimizer.zero_grad() \n",
    " \n",
    "        # forward + backward + optimize \n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        scheduler.step() \n",
    " \n",
    "#         # Keep stats for Loss and Accuracy \n",
    "#         running_loss += loss.item() \n",
    " \n",
    "        # Get the predicted class with the highest score \n",
    "        _, prediction = torch.max(outputs,1) \n",
    "        # Count of predictions that matched the target label \n",
    "        correct_prediction += (prediction == labels).sum().item() \n",
    "        total_prediction += prediction.shape[0]\n",
    "        \n",
    "#         truths = []\n",
    "#         preds = []\n",
    "#         labels = labels.cpu()\n",
    "#         prediction = prediction.cpu()\n",
    "# #         print(f'labes: {labels},preds: {prediction}')\n",
    "#         truths.append(labels)\n",
    "#         preds.append(prediction)\n",
    "#         loss2 = logf1_loss(truths, preds) \n",
    "#         loss1.backward() \n",
    "#         optimizer.step() \n",
    "#         scheduler.step() \n",
    " \n",
    "        # Keep stats for Loss and Accuracy \n",
    "        running_loss += loss.item()\n",
    " \n",
    "        # if i % 10 == 0:    # print every 10 mini-batches \n",
    "        #     print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10)) \n",
    " \n",
    "    # Print stats at the end of the epoch \n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches \n",
    "    acc = correct_prediction/total_prediction \n",
    "    losse_curves  = {\"Epoch\":epoch,\n",
    "                            \"Loss\":avg_loss,\n",
    "                             \"Accuracy\":acc,\n",
    "                             }\n",
    "    if epoch == 1:\n",
    "      print(\"create loss dict\")\n",
    "      loss_log_dict = {}\n",
    "      for k, v in losse_curves.items():\n",
    "        loss_log_dict[k] = []\n",
    "      print(\"loss dict created\")\n",
    "    \n",
    "    for k, v in loss_log_dict.items():\n",
    "      loss_log_dict[k].append(losse_curves[k])  # 把每batch的loss数据加入到 loss curves中\n",
    "    write_line2log(losse_curves, logfilepath, isprint=True)\n",
    "\n",
    "    ## 模型保存\n",
    "    if epoch % 50 == 0:\n",
    "      test_acc(myModel,val_dl)\n",
    "      save_model(epoch)\n",
    "    # print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}') \n",
    " \n",
    "  print('Finished Training') \n",
    " \n",
    "num_epochs=501   # Just for demo, adjust this higher. \n",
    "# \"\"\"获取大文件的数据\"\"\"\n",
    "# x_list, y_list=read_data()\n",
    "# \"\"\"划分为训练集和测试集及label文件\"\"\"\n",
    "# split_data(x_list,y_list)\n",
    "training(myModel, train_dl, num_epochs)\n",
    "# if __name__ == '__main__':\n",
    "#     \"\"\"获取大文件的数据\"\"\"\n",
    "#     x_list, y_list=read_data()\n",
    "#     \"\"\"划分为训练集和测试集及label文件\"\"\"\n",
    "#     split_data(x_list,y_list)\n",
    "#     training(myModel, train_dl, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models\n",
    "import torch\n",
    "import torchsummary\n",
    "\n",
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "torchsummary.summary(model.cuda(), (3, 224, 224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 138.36M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import nessi\n",
    "\n",
    "model = myModel\n",
    "nessi.get_model_size(model, 'torch' ,input_size=(32,2,64,430))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-b32921c9",
   "language": "python",
   "display_name": "PyCharm (桌面)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}